{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__7rnCG1Ig4_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b635d93c-797b-4959-f6da-9787523d98ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# Import nltk, os, json, re, shutil, pandas, numpy, scipy, matplotlib, seaborn\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import skew, kurtosis\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the folder containing the JSON files\n",
        "data_folder = \"10K_item1a_PERMNO_2015_sic_tic\"\n",
        "sampled_folder = \"sampled_files\"\n",
        "os.makedirs(sampled_folder, exist_ok=True)\n",
        "\n",
        "# Group files by year and then by industry\n",
        "industry_year_files = {}\n",
        "\n",
        "for file_name in os.listdir(data_folder):\n",
        "    if file_name.endswith(\".json\"):\n",
        "        file_path = os.path.join(data_folder, file_name)\n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Extract SIC code and year\n",
        "        sic_code = int(data['SIC'])\n",
        "        year = data['filing_date'][:4]  # Extract year from filing_date\n",
        "\n",
        "        # Determine industry based on SIC code\n",
        "        if 1 <= sic_code <= 999:\n",
        "            industry = \"Agriculture, Forestry and Fishing\"\n",
        "        elif 1000 <= sic_code <= 1499:\n",
        "            industry = \"Mining\"\n",
        "        elif 1500 <= sic_code <= 1799:\n",
        "            industry = \"Construction\"\n",
        "        elif 2000 <= sic_code <= 3999:\n",
        "            industry = \"Manufacturing\"\n",
        "        elif 4000 <= sic_code <= 4999:\n",
        "            industry = \"Transportation and other Utilities\"\n",
        "        elif 5000 <= sic_code <= 5199:\n",
        "            industry = \"Wholesale Trade\"\n",
        "        elif 5200 <= sic_code <= 5999:\n",
        "            industry = \"Retail Trade\"\n",
        "        elif 6000 <= sic_code <= 6799:\n",
        "            industry = \"Finance, Insurance and Real Estate\"\n",
        "        elif 7000 <= sic_code <= 8999:\n",
        "            industry = \"Services\"\n",
        "        elif 9000 <= sic_code <= 9999:\n",
        "            industry = \"Public Administration\"\n",
        "        else:\n",
        "            industry = \"Unknown\"\n",
        "\n",
        "        # Group files by (year, industry)\n",
        "        key = (year, industry)\n",
        "        if key not in industry_year_files:\n",
        "            industry_year_files[key] = []\n",
        "        industry_year_files[key].append((file_name, sic_code))\n",
        "\n",
        "# Sort files within each (year, industry) group and sample 10 files per group\n",
        "for (year, industry), files in sorted(industry_year_files.items()):  # Sort by year and then industry\n",
        "    sampled_files = sorted(files, key=lambda x: x[0])[:10]  # Take the first 10 files based on file_name\n",
        "    for file_name, sic_code in sampled_files:\n",
        "        src_path = os.path.join(data_folder, file_name)\n",
        "\n",
        "        # Rename the file before copying to avoid duplicates\n",
        "        new_file_name = f\"{year}_{industry.replace(' ', '_')}_{file_name}\"\n",
        "        dest_path = os.path.join(sampled_folder, new_file_name)\n",
        "\n",
        "        shutil.copy(src_path, dest_path)\n",
        "\n",
        "print(f\"Sampled files have been renamed and saved in the folder: {sampled_folder}\")"
      ],
      "metadata": {
        "id": "VKS9fzaVOXyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the output folder exists\n",
        "sampled_folder = \"sampled_files\"\n",
        "output_folder = \"extracted_sentences\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Define relevant and irrelevant keywords for each category\n",
        "keywords = {\n",
        "    \"Attack\": {\n",
        "        \"relevant\": [\"cyber-\", \"cyber\", \"networks\", \"systems\", \"products\", \"services\", \"datacenter\", \"infrastructure\"],\n",
        "        \"irrelevant\": [\"terror\", \"war\", \"contraband\", \"bombs\"]\n",
        "    },\n",
        "    \"Threat\": {\n",
        "        \"relevant\": [\"cyber-\", \"cyber\", \"networks\", \"systems\", \"products\", \"services\", \"datacenter\", \"infrastructure\"],\n",
        "        \"irrelevant\": [\"terror\", \"simulator\", \"disease\", \"legal action\", \"competitors\"]\n",
        "    },\n",
        "    \"Computer, information, system\": {\n",
        "        \"relevant\": [\"malware\", \"virus\", \"viruses\", \"intrusions\"],\n",
        "        \"irrelevant\": [\"fires\", \"product sales\", \"warranty claim\"]\n",
        "    },\n",
        "    \"Malicious\": {\n",
        "        \"relevant\": [\"software\", \"programs\", \"third parties\", \"attacks\"],\n",
        "        \"irrelevant\": []\n",
        "    },\n",
        "    \"Breaches\": {\n",
        "        \"relevant\": [],\n",
        "        \"irrelevant\": [\"fiduciary duty\", \"fiduciary duties\", \"covenant\", \"credit\", \"agreement\"]\n",
        "    },\n",
        "    \"Hacker, hacking, social engineering, denial of service, cyberattack, cybersecurity\": {\n",
        "        \"relevant\": [\"hacker\", \"hacking\", \"social engineering\", \"denial of service\", \"cyberattack\", \"cybersecurity\"],\n",
        "        \"irrelevant\": [\"fiduciary\", \"warranty\", \"regulations\", \"contract\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to generate regex patterns dynamically for keyword variations\n",
        "def keyword_pattern(word):\n",
        "    \"\"\"\n",
        "    Generate regex pattern to match variations of a word dynamically.\n",
        "    Example: \"attack\" -> r\"\\battack(s|ing|ed)?\\b\"\n",
        "    \"\"\"\n",
        "    lemma = lemmatizer.lemmatize(word)  # Normalize the word\n",
        "    return rf\"\\b{re.escape(lemma)}(s|ing|ed)?\\b\"\n",
        "\n",
        "# Function to classify a sentence as relevant or irrelevant dynamically\n",
        "def classify_sentence(sentence, keywords):\n",
        "    sentence = sentence.lower()  # Convert to lowercase for case-insensitive matching\n",
        "\n",
        "    # Iterate through each category in the keywords dictionary\n",
        "    for category, terms in keywords.items():\n",
        "        # Check if the category keyword exists in the sentence\n",
        "        if category.lower() in sentence:\n",
        "            # Check for relevant keywords dynamically\n",
        "            if any(re.search(keyword_pattern(term), sentence) for term in terms[\"relevant\"]):\n",
        "                return True  # Relevant if any relevant keyword is found\n",
        "\n",
        "            # Check for irrelevant keywords dynamically\n",
        "            if any(re.search(keyword_pattern(term), sentence) for term in terms[\"irrelevant\"]):\n",
        "                continue  # Skip to the next category if irrelevant keywords are found\n",
        "\n",
        "    return False  # Irrelevant if no relevant keywords are found\n",
        "\n",
        "# Process each JSON file in the sampled folder\n",
        "for file_name in os.listdir(sampled_folder):\n",
        "    if file_name.endswith(\".json\"):\n",
        "        file_path = os.path.join(sampled_folder, file_name)\n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Extract the \"item_1A\" section and tokenize into sentences\n",
        "        item_1A_text = data.get(\"item_1A\", \"\")\n",
        "        sentences = sent_tokenize(item_1A_text)\n",
        "\n",
        "        # Classify sentences\n",
        "        relevant_sentences = []\n",
        "        for sentence in sentences:\n",
        "            if classify_sentence(sentence, keywords):\n",
        "                relevant_sentences.append(sentence)\n",
        "\n",
        "        # Calculate cybersecurity risk measures\n",
        "        total_sentences = len(sentences)\n",
        "        # risk_measure_ratio = len(relevant_sentences) / total_sentences if total_sentences > 0 else 0\n",
        "        risk_measure_count = len(relevant_sentences)\n",
        "\n",
        "        # Save relevant sentences and risk measures to a new JSON file\n",
        "        output_data = {\n",
        "            \"cik\": data.get(\"cik\"),\n",
        "            \"company\": data.get(\"company\"),\n",
        "            \"filing_date\": data.get(\"filing_date\"),\n",
        "            \"sic\": data.get(\"SIC\"),  # Include the SIC code in the output\n",
        "            \"relevant_sentences\": relevant_sentences,\n",
        "            # \"cybersecurity_risk_ratio\": risk_measure_ratio,\n",
        "            \"cybersecurity_risk_count\": risk_measure_count\n",
        "        }\n",
        "        output_file_path = os.path.join(output_folder, f\"relevant_{file_name}\")\n",
        "        with open(output_file_path, 'w') as output_file:\n",
        "            json.dump(output_data, output_file, indent=4)\n",
        "\n",
        "print(f\"Relevant sentences and cybersecurity risk measures saved in the folder: {output_folder}\")"
      ],
      "metadata": {
        "id": "ogwpQjjMObTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the folder containing the extracted JSON files\n",
        "output_folder = \"extracted_sentences\"\n",
        "\n",
        "# Load all JSON files into a DataFrame\n",
        "data = []\n",
        "for file_name in os.listdir(output_folder):\n",
        "    if file_name.endswith(\".json\"):\n",
        "        file_path = os.path.join(output_folder, file_name)\n",
        "        with open(file_path, 'r') as f:\n",
        "            file_data = json.load(f)\n",
        "            # Extract relevant fields\n",
        "            data.append({\n",
        "                \"cik\": file_data.get(\"cik\"),\n",
        "                \"company\": file_data.get(\"company\"),\n",
        "                \"filing_date\": file_data.get(\"filing_date\"),\n",
        "                \"sic\": int(file_data.get(\"sic\", 0)) if file_data.get(\"sic\") else None,\n",
        "                # \"cybersecurity_risk_ratio\": file_data.get(\"cybersecurity_risk_ratio\", 0),\n",
        "                \"cybersecurity_risk_count\": file_data.get(\"cybersecurity_risk_count\", 0)\n",
        "            })\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Add a \"year\" column\n",
        "df['year'] = pd.to_datetime(df['filing_date']).dt.year\n",
        "\n",
        "# Define SIC code ranges for industries\n",
        "sic_ranges = {\n",
        "    \"Agriculture, Forestry and Fishing\": (1, 999),\n",
        "    \"Mining\": (1000, 1499),\n",
        "    \"Construction\": (1500, 1799),\n",
        "    \"Manufacturing\": (2000, 3999),\n",
        "    \"Transportation and other Utilities\": (4000, 4999),\n",
        "    \"Wholesale Trade\": (5000, 5199),\n",
        "    \"Retail Trade\": (5200, 5999),\n",
        "    \"Finance, Insurance and Real Estate\": (6000, 6799),\n",
        "    \"Services\": (7000, 8999),\n",
        "    \"Public Administration\": (9000, 9999)\n",
        "}\n",
        "\n",
        "# Function to map SIC codes to industries\n",
        "def map_sic_to_industry(sic):\n",
        "    if pd.isnull(sic):  # Handle missing SIC codes\n",
        "        return \"Unknown\"\n",
        "    for industry, (low, high) in sic_ranges.items():\n",
        "        if low <= sic <= high:\n",
        "            return industry\n",
        "    return \"Unknown\"  # Handle SIC codes outside the defined ranges\n",
        "\n",
        "# Map SIC codes to industries\n",
        "df['industry'] = df['sic'].apply(map_sic_to_industry)\n",
        "\n",
        "# Debugging: Check unique industries\n",
        "print(\"Unique industries:\", df['industry'].unique())\n",
        "\n",
        "# Part 1: Compute Descriptive Statistics by Industry\n",
        "descriptive_stats = []\n",
        "for industry, group in df.groupby('industry'):\n",
        "    stats = {\n",
        "        \"industry\": industry,\n",
        "        \"N\": len(group),\n",
        "        \"mean\": group['cybersecurity_risk_count'].mean(),\n",
        "        \"std_dev\": group['cybersecurity_risk_count'].std(),\n",
        "        \"skewness\": skew(group['cybersecurity_risk_count']),\n",
        "        \"kurtosis\": kurtosis(group['cybersecurity_risk_count']),\n",
        "        \"min\": group['cybersecurity_risk_count'].min(),\n",
        "        \"max\": group['cybersecurity_risk_count'].max(),\n",
        "        \"1%\": group['cybersecurity_risk_count'].quantile(0.01),\n",
        "        \"5%\": group['cybersecurity_risk_count'].quantile(0.05),\n",
        "        \"25%\": group['cybersecurity_risk_count'].quantile(0.25),\n",
        "        \"50%\": group['cybersecurity_risk_count'].quantile(0.50),\n",
        "        \"75%\": group['cybersecurity_risk_count'].quantile(0.75),\n",
        "        \"95%\": group['cybersecurity_risk_count'].quantile(0.95),\n",
        "        \"99%\": group['cybersecurity_risk_count'].quantile(0.99)\n",
        "    }\n",
        "    descriptive_stats.append(stats)\n",
        "\n",
        "# Convert descriptive stats to DataFrame\n",
        "descriptive_stats_df = pd.DataFrame(descriptive_stats)\n",
        "\n",
        "# Save descriptive stats to a CSV file\n",
        "descriptive_stats_df.to_csv(\"descriptive_stats_by_industry.csv\", index=False)\n",
        "\n",
        "# Part 2: Compute Mean and Standard Deviation by Industry and Year\n",
        "mean_std_by_year = df.groupby(['industry', 'year'])['cybersecurity_risk_count'].agg(['mean', 'std']).reset_index()\n",
        "\n",
        "# Save mean and std deviation by year to a CSV file\n",
        "mean_std_by_year.to_csv(\"mean_std_by_industry_year.csv\", index=False)\n",
        "\n",
        "# Debugging: Check the grouped DataFrame\n",
        "print(\"Grouped DataFrame:\")\n",
        "print(mean_std_by_year.head())\n",
        "\n",
        "# Part 3: Plot Trends Over Time\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.lineplot(data=mean_std_by_year, x='year', y='mean', hue='industry', marker='o')\n",
        "plt.title(\"Mean Cybersecurity Risk Count by Industry (2015-2023)\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Mean Cybersecurity Risk Count\")\n",
        "plt.legend(title=\"Industry\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"cybersecurity_risk_trends.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X7c90yx8OfG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[GDrive Link to access Plot Figure, CSV files for Descriptive Statistics & Mean + Standard Deviation](https://drive.google.com/drive/folders/1vqm09ZhhGAJCAwt-GFz4i_SKuJ9bwYGU?usp=sharing)"
      ],
      "metadata": {
        "id": "gDCVrihuBmpO"
      }
    }
  ]
}